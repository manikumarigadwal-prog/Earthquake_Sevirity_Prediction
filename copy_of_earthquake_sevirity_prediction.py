# -*- coding: utf-8 -*-
"""Copy of earthquake_sevirity_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18opAswD9aI-KkXvxas2jO476J_i_eAmF
"""

import pandas as pd
import numpy as np
import joblib

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

df = pd.read_csv("/content/archive.zip")

print("Dataset Shape:", df.shape)
print(df.head())

print(df.columns.tolist())

def mag_to_severity(mag):
    if pd.isna(mag):
        return np.nan
    m = float(mag)
    if m < 4.0:
        return "Low"
    elif m < 5.0:
        return "Moderate"
    elif m < 6.0:
        return "Strong"
    else:
        return "Severe"

df["severity"] = df["magnitude"].apply(mag_to_severity)

# Drop rows missing key info
df = df.dropna(subset=["magnitude", "tsunami"])

# Use all numeric columns relevant to prediction
features = ["magnitude", "cdi", "mmi", "sig", "nst", "dmin",
            "gap", "depth", "latitude", "longitude", "Year", "Month"]

# Keep only columns that actually exist in your df (safety)
features = [c for c in features if c in df.columns]

X = df[features]
y_sev = df["severity"]
y_tsu = df["tsunami"].astype(int)

num_transformer = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

cat_transformer = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer([
    ("num", num_transformer, X.select_dtypes(include=["float64", "int64"]).columns),
    ("cat", cat_transformer, X.select_dtypes(include=["object"]).columns)
])

X_train, X_test, y_sev_train, y_sev_test, y_tsu_train, y_tsu_test = train_test_split(
    X, y_sev, y_tsu, test_size=0.2, random_state=42
)

sev_model = Pipeline([
    ("preprocessor", preprocessor),
    ("classifier", RandomForestClassifier(n_estimators=200, random_state=42))
])

sev_model.fit(X_train, y_sev_train)
sev_pred = sev_model.predict(X_test)

print("Feature columns used:", features)
print("Severity distribution:\n", y_sev.value_counts())
print("Tsunami distribution:\n", y_tsu.value_counts())

# Numeric impute + scale
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

preprocessor = ColumnTransformer(transformers=[
    ("num", numeric_transformer, features)
], remainder="drop")

# Stratify by tsunami to keep class proportions (binary)
X_train, X_test, y_sev_train, y_sev_test, y_tsu_train, y_tsu_test = train_test_split(
    X, y_sev, y_tsu, test_size=0.20, random_state=42, stratify=y_tsu
)

# Severity model (Random Forest)
sev_pipeline = Pipeline(steps=[
    ("preproc", preprocessor),
    ("clf", RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1))
])

print("\nTraining severity model...")
sev_pipeline.fit(X_train, y_sev_train)

y_sev_pred = sev_pipeline.predict(X_test)
print("\nSeverity evaluation")
print("Accuracy:", accuracy_score(y_sev_test, y_sev_pred))
print(classification_report(y_sev_test, y_sev_pred))
print("Confusion matrix:\n", confusion_matrix(y_sev_test, y_sev_pred))

joblib.dump(sev_pipeline, "severity_model_kaggle.joblib")
print("Saved severity_model_kaggle.joblib")

# Tsunami model (Logistic Regression)
tsu_pipeline = Pipeline(steps=[
    ("preproc", preprocessor),
    ("clf", LogisticRegression(max_iter=2000, solver="liblinear"))
])

print("\nTraining tsunami model...")
tsu_pipeline.fit(X_train, y_tsu_train)

y_tsu_pred = tsu_pipeline.predict(X_test)
print("\nTsunami evaluation")
print("Accuracy:", accuracy_score(y_tsu_test, y_tsu_pred))
print(classification_report(y_tsu_test, y_tsu_pred))
print("Confusion matrix:\n", confusion_matrix(y_tsu_test, y_tsu_pred))

joblib.dump(tsu_pipeline, "tsunami_model_kaggle.joblib")
print("Saved tsunami_model_kaggle.joblib")

def predict_single(sample: dict):
    """
    sample keys: any subset of `features`. Missing fields will be treated as NaN
    Example:
      sample = {
        "magnitude": 6.8, "cdi": 5, "mmi": 5, "sig": 700,
        "nst": 120, "dmin": 1.2, "gap": 25, "depth": 15,
        "latitude": -9.7, "longitude": 159.6, "Year": 2022, "Month": 11
      }
    """
    row = {c: sample.get(c, np.nan) for c in features}
    sample_df = pd.DataFrame([row])
    sev = sev_pipeline.predict(sample_df)[0]
    tsu = int(tsu_pipeline.predict(sample_df)[0])
    tsu_prob = float(tsu_pipeline.predict_proba(sample_df)[0, 1]) if hasattr(tsu_pipeline, "predict_proba") else None
    return {"severity": sev, "tsunami": tsu, "tsunami_prob": tsu_prob}

# Example usage using medians from dataset with a higher magnitude:
example = {c: float(X[c].median()) for c in features}
example["magnitude"] = 6.8
example["depth"] = 15.0
print("\nSample prediction for example input:")
print(predict_single(example))

